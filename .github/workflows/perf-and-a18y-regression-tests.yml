name: Daily Performance & Accessibility Audit

on:
  schedule:
    # Runs at 00:00 UTC every day
    - cron: '0 0 * * *'
  workflow_dispatch:

permissions:
  contents: write

env:
  # UPDATE THIS TO YOUR WEBSITE URL
  SITE_URL: 'https://store.pricedb.io'
  # The folder where history will be stored in the repo
  HISTORY_DIR: 'hist'
  # LIMIT PAGES FOR DEBUGGING (Set to a high number like 500 for production)
  MAX_PAGES: 10

jobs:
  audit-and-compare:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install Unlighthouse
        run: npm install -g @unlighthouse/cli puppeteer

      - name: Create Unlighthouse Config
        # We generate a config file to handle samples, mobile, and limits cleanly
        run: |
          cat <<EOF > unlighthouse.config.ts
          export default {
            site: '${{ env.SITE_URL }}',
            scanner: {
              device: 'mobile',
              samples: 3, # Run 3 passes to reduce noise
              maxRoutes: ${{ env.MAX_PAGES }}, # Limit number of pages scanned
            },
            discovery: {
              maxPages: ${{ env.MAX_PAGES }}, # Stop crawling after finding this many pages
            },
            debug: true
          }
          EOF

      - name: Run Unlighthouse CI
        # Runs using the config file we just created
        run: unlighthouse-ci --reporter json

      - name: Process Results & Compare History
        id: process
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // --- Configuration ---
            const historyDir = process.env.HISTORY_DIR;
            const unlighthouseDir = '.unlighthouse';
            const threshold = 2; // Regression threshold points

            // --- Helpers ---
            const formatDate = (date) => date.toISOString().split('T')[0];
            const getScore = (val) => Math.round((val || 0) * 100);

            // Ensure history directory exists
            if (!fs.existsSync(historyDir)) {
              fs.mkdirSync(historyDir, { recursive: true });
            }

            // 1. Read Unlighthouse JSON Output
            let rawData = [];
            try {
              // Usually in .unlighthouse/ci-result.json
              const jsonPath = path.join(unlighthouseDir, 'ci-result.json');
              
              if (!fs.existsSync(jsonPath)) {
                 // Fallback: sometimes it's unlighthouse.json or in a subdir depending on version
                 const altPath = path.join(unlighthouseDir, 'unlighthouse.json');
                 if(fs.existsSync(altPath)) {
                    console.log(`Found results at ${altPath}`);
                    const content = JSON.parse(fs.readFileSync(altPath, 'utf8'));
                    rawData = content.routes || [];
                 } else {
                    throw new Error(`Could not find ci-result.json in ${unlighthouseDir}`);
                 }
              } else {
                 console.log(`Found results at ${jsonPath}`);
                 const content = JSON.parse(fs.readFileSync(jsonPath, 'utf8'));
                 // Support different JSON structures (array vs object with routes property)
                 rawData = Array.isArray(content) ? content : (content.routes || []);
              }
            } catch (e) {
              core.setFailed(`Failed to read Unlighthouse results: ${e.message}`);
              return;
            }

            if (!rawData || rawData.length === 0) {
              core.setFailed("No routes found in Unlighthouse output.");
              return;
            }

            // 2. Convert to CSV format
            // FIX: Filter out routes where scan failed (scores is undefined)
            const validRoutes = rawData.filter(r => r.scores && r.path);
            
            console.log(`Processing ${validRoutes.length} valid scanned routes (out of ${rawData.length} total found).`);

            const todayData = validRoutes.map(route => ({
              path: route.path,
              performance: getScore(route.scores.performance),
              accessibility: getScore(route.scores.accessibility),
              seo: getScore(route.scores.seo),
              bestPractices: getScore(route.scores['best-practices'])
            }));

            // Generate CSV String
            const header = 'Path,Performance,Accessibility,SEO,Best Practices\n';
            const csvRows = todayData.map(r => 
              `${r.path},${r.performance},${r.accessibility},${r.seo},${r.bestPractices}`
            ).join('\n');
            
            const today = new Date();
            const todayStr = formatDate(today);
            const todayFile = path.join(historyDir, `${todayStr}.csv`);
            
            // Save Today's CSV
            fs.writeFileSync(todayFile, header + csvRows);
            console.log(`Saved results to ${todayFile}`);

            // 3. Load Historical Data
            const getPastDate = (daysAgo) => {
              const d = new Date();
              d.setDate(d.getDate() - daysAgo);
              return formatDate(d);
            };

            const loadCsvMap = (dateStr) => {
              const file = path.join(historyDir, `${dateStr}.csv`);
              if (!fs.existsSync(file)) {
                console.log(`No history file found for ${dateStr}`);
                return null;
              }
              
              const content = fs.readFileSync(file, 'utf8').trim().split('\n');
              const map = new Map();
              // Skip header, parse rows
              content.slice(1).forEach(row => {
                const cols = row.split(',');
                if (cols.length >= 3) {
                  // key: path, value: { perf, acc }
                  map.set(cols[0], { 
                    perf: parseInt(cols[1]), 
                    acc: parseInt(cols[2]) 
                  });
                }
              });
              return map;
            };

            const yesterdayStr = getPastDate(1);
            const lastWeekStr = getPastDate(7);

            console.log(`Comparing against Yesterday (${yesterdayStr}) and Last Week (${lastWeekStr})`);

            const yesterdayMap = loadCsvMap(yesterdayStr);
            const lastWeekMap = loadCsvMap(lastWeekStr);

            // 4. Compare and Report
            let markdownReport = `### ðŸ“‰ Performance Regression Report (${todayStr})\n\n`;
            let regressionCount = 0;

            const checkRegression = (url, metricName, current, old, dateLabel) => {
              // Ensure we have valid numbers to compare
              if (isNaN(current) || isNaN(old)) return;
              
              const diff = old - current;
              if (diff > threshold) {
                regressionCount++;
                markdownReport += `- âš ï¸ **${url}** - ${metricName} dropped by **${diff}** points vs ${dateLabel} (${old} -> ${current})\n`;
              }
            };

            todayData.forEach(page => {
              // Compare Yesterday
              if (yesterdayMap && yesterdayMap.has(page.path)) {
                const y = yesterdayMap.get(page.path);
                checkRegression(page.path, 'Performance', page.performance, y.perf, 'Yesterday');
                checkRegression(page.path, 'Accessibility', page.accessibility, y.acc, 'Yesterday');
              }

              // Compare Last Week
              if (lastWeekMap && lastWeekMap.has(page.path)) {
                const w = lastWeekMap.get(page.path);
                checkRegression(page.path, 'Performance', page.performance, w.perf, 'Last Week');
                checkRegression(page.path, 'Accessibility', page.accessibility, w.acc, 'Last Week');
              }
            });

            if (regressionCount === 0) {
              markdownReport += "âœ… No significant regressions detected compared to yesterday or last week.";
            } else {
                markdownReport += `\n**Total Regressions Found:** ${regressionCount}`;
            }

            // Output to Job Summary
            await core.summary.addRaw(markdownReport).write();
            
            // Set output for the artifact step
            core.setOutput('today_file', todayFile);

      - name: Commit CSV to Repo
        run: |
          git config --global user.name "GitHub Action Runner"
          git config --global user.email "actions@github.com"
          git add ${{ env.HISTORY_DIR }}/*.csv
          # Only commit if there are changes to avoid empty commit errors
          git diff --quiet && git diff --staged --quiet || (git commit -m "Add performance audit for $(date +'%Y-%m-%d')" && git push)

      - name: Upload CSV Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-audit-${{ env.MAX_PAGES }}-pages
          path: ${{ steps.process.outputs.today_file }}
